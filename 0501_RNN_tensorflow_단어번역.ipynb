{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transpose 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 7)\n",
      "(5, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "ct = np.arange(7*5*2).reshape(7,5,2)\n",
    "\n",
    "print(np.transpose(ct).shape)\n",
    "print(np.transpose(ct, [1,2,0]).shape)\n",
    "\n",
    "# m*n -> transpose => n*m\n",
    "# i*j*k => transpose => 다양한 형태로 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 14 15 16]\n",
      " [17 18 19 20]\n",
      " [21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "t = [i for i in range(1,25)]\n",
    "t = np.reshape(t,[2,3,4])\n",
    "t = t[-1]\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case study 1 : 4글자로 구성된 단어 -> 학습\n",
    "# 단어의 앞 3글자 입력 => 마지막 글자 예측\n",
    "'''\n",
    "wood => 학습 => 모델\n",
    "woo 입력 =====> 모델 => d 예측\n",
    "wop 입력 =====> 모델 => d 예측\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr함수 : 10진수로 ASCII코드 불러내기\n",
    "char_arr = [chr(i) for i in range(97,123)]\n",
    "\n",
    "num_dic = {}\n",
    "num = 0\n",
    "for i in char_arr :\n",
    "    num_dic[i] = num\n",
    "    num += 1\n",
    "num_dic\n",
    "\n",
    "num_dic={n:i for i, n in enumerate(char_arr)}\n",
    "num_dic\n",
    "\n",
    "dic_len = len(num_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = ['word','wood','deep','dive','cold','cool','load','love','kiss','kind']\n",
    "\n",
    "# < 목표 >\n",
    "# x:wor, y = d\n",
    "# x:woo, y = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq 데이터를 전달받아 원핫인코딩 한 후 앞 세글자를 x로 뒤 한 글자를 y로 \n",
    "\n",
    "def make_batch(seq_data) :\n",
    "    \n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data :\n",
    "        my_input =[num_dic[n] for n in seq[:-1]]\n",
    "        target = num_dic[seq[-1]]\n",
    "        \n",
    "        # 원핫인코딩\n",
    "        input_batch.append(np.eye(dic_len)[my_input])\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, target_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 15, 4, 3, 11, 3, 4, 18, 3]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_corss_entropy_with_logist 함수는 label이 원핫 인코딩 돼있을 때\n",
    "# sparse_softmax_corss_entropy_with_logist 함수는 label이 원핫 인코딩 안 됐을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수\n",
    "# lr=0.01, n_hidden=128, epoch=30, n_steps=3, n_input=n_class=26=dic_len\n",
    "lr = 0.01\n",
    "n_hidden = 128   # 출력 셀의 개수\n",
    "total_epoch = 30\n",
    "n_steps = 3      # 입력 3글자 (일반적으로 가장 긴 단어의 글자수를 주고 나머지는 zero padding)\n",
    "n_input = n_class = dic_len  # 입력과 출력값의 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# 신경망 모델 구성                     단어수   입력글자수  26개의 종류\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.int32, shape=[None])  # => [3], [5] .. \n",
    "# y가 원핫인코딩이 돼있다면        shape=[None, n_class] => [0,0,0,1,0,0...]\n",
    "\n",
    "w = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, 0.5)  # Drop out 옵션 (오버피팅 방지)\n",
    "\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cell, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.transpose(outputs, [1,0,2])\n",
    "# 10,3,128 => 3,10,128\n",
    "outputs = outputs[-1]  # 10, 128\n",
    "\n",
    "model = tf.matmul(outputs, w) + b\n",
    "# tf.matmul([10,128],[128,26]) => [10,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "opt = tf.train.AdamOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭 : 0001 비용 : 2.92489\n",
      "에폭 : 0002 비용 : 2.20015\n",
      "에폭 : 0003 비용 : 2.01612\n",
      "에폭 : 0004 비용 : 1.16369\n",
      "에폭 : 0005 비용 : 1.11578\n",
      "에폭 : 0006 비용 : 0.53239\n",
      "에폭 : 0007 비용 : 1.37035\n",
      "에폭 : 0008 비용 : 1.38564\n",
      "에폭 : 0009 비용 : 1.27288\n",
      "에폭 : 0010 비용 : 1.35574\n",
      "에폭 : 0011 비용 : 0.98233\n",
      "에폭 : 0012 비용 : 0.85629\n",
      "에폭 : 0013 비용 : 0.51626\n",
      "에폭 : 0014 비용 : 0.54353\n",
      "에폭 : 0015 비용 : 0.70600\n",
      "에폭 : 0016 비용 : 0.63348\n",
      "에폭 : 0017 비용 : 0.64992\n",
      "에폭 : 0018 비용 : 0.53844\n",
      "에폭 : 0019 비용 : 1.10667\n",
      "에폭 : 0020 비용 : 0.31352\n",
      "에폭 : 0021 비용 : 0.33166\n",
      "에폭 : 0022 비용 : 0.44037\n",
      "에폭 : 0023 비용 : 0.67002\n",
      "에폭 : 0024 비용 : 0.67361\n",
      "에폭 : 0025 비용 : 0.31416\n",
      "에폭 : 0026 비용 : 0.70809\n",
      "에폭 : 0027 비용 : 0.53130\n",
      "에폭 : 0028 비용 : 0.81074\n",
      "에폭 : 0029 비용 : 0.50393\n",
      "에폭 : 0030 비용 : 0.95269\n",
      "모델작성 완료\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(total_epoch) :\n",
    "    _, cv = sess.run([opt,cost], feed_dict={x:input_batch, y:target_batch})\n",
    "    \n",
    "    print('에폭 : %04d'%(epoch+1), '비용 : {:.5f}'.format(cv))\n",
    "    \n",
    "print('모델작성 완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 노드\n",
    "pred = tf.cast(tf.argmax(model,1), tf.int32)\n",
    "\n",
    "# 결과 비교\n",
    "predCheck = tf.equal(pred, y)\n",
    "\n",
    "# 정확도\n",
    "accuracy = tf.reduce_mean(tf.cast(predCheck, tf.float32))\n",
    "\n",
    "# input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "pv, av = sess.run([pred, accuracy], feed_dict={x:input_batch, y:target_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측결과\n",
      "\n",
      "입력값 :  ['wor', 'woo', 'dee', 'div', 'col', 'coo', 'loa', 'lov', 'kis', 'kin']\n",
      "예측값 :  ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'loae', 'lovd', 'kiss', 'kind']\n",
      "정확도 :  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_words = []\n",
    "for i, v in enumerate(seq_data) :\n",
    "    last_char = char_arr[pv[i]]\n",
    "    predict_words.append(v[:3]+last_char)\n",
    "print('예측결과\\n')\n",
    "print('입력값 : ',[w[:3] for w in seq_data])\n",
    "print('예측값 : ',predict_words)\n",
    "print('정확도 : ',av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_test = ['deap','luve']\n",
    "input_x, target_y = make_batch(seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측결과\n",
      "\n",
      "입력값 :  ['dea', 'luv']\n",
      "예측값 :  ['deap', 'luve']\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "pv, av = sess.run([pred, accuracy], feed_dict={x:input_x, y:target_y})\n",
    "\n",
    "predict_words = []\n",
    "for i, v in enumerate(seq_test) :\n",
    "    last_char = char_arr[pv[i]]\n",
    "    predict_words.append(v[:3]+last_char)\n",
    "print('예측결과\\n')\n",
    "print('입력값 : ',[w[:3] for w in seq_test])\n",
    "print('예측값 : ',predict_words)\n",
    "print('정확도 : ',av)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq : 기계번역\n",
    "\n",
    " : 입력신경망(Encoder) / 출력신경망(Decoder)\n",
    "ex) 나는 학교에 간다 => I go to school\n",
    "\n",
    ": 챗봇, 번역, 이미지 캡셔닝에서 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "S : 디코딩 입력의 시작\n",
    "E : 디코딩 출력의 끝\n",
    "P : 현재 배치되는 데이터의 time step 크기보다 작은 경우 빈 시퀀스를 채우는 심볼\n",
    "\n",
    "배치 데이터의 최대 크기 4인 경우\n",
    "word = ['w','o','r','d']\n",
    "to   = ['t','o','P','P']  # 길이 4에 맞춰 패딩됨\n",
    "'''\n",
    "\n",
    "# 학습에 사용할 단어 배열\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']  # 일반적으론 corpus\n",
    "\n",
    "num_dic = {n:i for i,n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)  # 41\n",
    "\n",
    "seq_data = [['word', '단어'], ['wood', '나무'], ['game', '놀이'],\n",
    "            ['girl', '소녀'], ['kiss', '키스'], ['love', '사랑']]\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data: # ['word', '단어']\n",
    "        inputdata = [num_dic[n] for n in seq[0]]  # word\n",
    "        outputdata = [num_dic[n] for n in ('S' + seq[1])]  # S단어\n",
    "        targetdata = [num_dic[n] for n in (seq[1] + 'E')]  # 단어E\n",
    "\n",
    "        input_batch.append(np.eye(dic_len)[inputdata])\n",
    "        output_batch.append(np.eye(dic_len)[outputdata])\n",
    "        target_batch.append(targetdata)\n",
    "\n",
    "    return input_batch, output_batch, target_batch\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 모델 옵션설정\n",
    "lr = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "n_input = n_class = dic_len  # 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성 = [배치사이즈, 단계, 입력크기]\n",
    "encInput = tf.placeholder(tf.float32, [None, None, n_input])   # word\n",
    "decInput = tf.placeholder(tf.float32, [None, None, n_input])  # S단어\n",
    "targets = tf.placeholder(tf.int64, [None, None])              # 단어E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 셀을 구성\n",
    "with tf.variable_scope('encode') :\n",
    "    enc_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # 위로       옆으로\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, encInput, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# 디코더 셀 구성\n",
    "with tf.variable_scope('decode') :\n",
    "    dec_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    #                                                                입력에서 넘어온 값\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, decInput, initial_state=enc_states, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-333-d301db4b0784>:1: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "\n",
    "opt = tf.train.AdamOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭 : 0001 비용 : 3.71407\n",
      "에폭 : 0002 비용 : 3.59273\n",
      "에폭 : 0003 비용 : 3.42726\n",
      "에폭 : 0004 비용 : 3.02099\n",
      "에폭 : 0005 비용 : 2.33814\n",
      "에폭 : 0006 비용 : 2.35971\n",
      "에폭 : 0007 비용 : 2.18335\n",
      "에폭 : 0008 비용 : 1.71671\n",
      "에폭 : 0009 비용 : 1.67572\n",
      "에폭 : 0010 비용 : 1.82902\n",
      "에폭 : 0011 비용 : 1.60616\n",
      "에폭 : 0012 비용 : 1.30773\n",
      "에폭 : 0013 비용 : 1.19909\n",
      "에폭 : 0014 비용 : 0.94755\n",
      "에폭 : 0015 비용 : 0.90957\n",
      "에폭 : 0016 비용 : 0.92235\n",
      "에폭 : 0017 비용 : 0.69990\n",
      "에폭 : 0018 비용 : 0.76908\n",
      "에폭 : 0019 비용 : 0.59338\n",
      "에폭 : 0020 비용 : 0.76664\n",
      "에폭 : 0021 비용 : 0.61363\n",
      "에폭 : 0022 비용 : 0.47909\n",
      "에폭 : 0023 비용 : 0.51138\n",
      "에폭 : 0024 비용 : 0.41701\n",
      "에폭 : 0025 비용 : 0.56903\n",
      "에폭 : 0026 비용 : 0.32239\n",
      "에폭 : 0027 비용 : 0.26971\n",
      "에폭 : 0028 비용 : 0.28315\n",
      "에폭 : 0029 비용 : 0.28808\n",
      "에폭 : 0030 비용 : 0.19545\n",
      "에폭 : 0031 비용 : 0.13966\n",
      "에폭 : 0032 비용 : 0.13641\n",
      "에폭 : 0033 비용 : 0.13492\n",
      "에폭 : 0034 비용 : 0.14593\n",
      "에폭 : 0035 비용 : 0.13029\n",
      "에폭 : 0036 비용 : 0.12888\n",
      "에폭 : 0037 비용 : 0.07538\n",
      "에폭 : 0038 비용 : 0.11554\n",
      "에폭 : 0039 비용 : 0.09771\n",
      "에폭 : 0040 비용 : 0.05219\n",
      "에폭 : 0041 비용 : 0.06621\n",
      "에폭 : 0042 비용 : 0.03052\n",
      "에폭 : 0043 비용 : 0.09894\n",
      "에폭 : 0044 비용 : 0.04164\n",
      "에폭 : 0045 비용 : 0.08582\n",
      "에폭 : 0046 비용 : 0.02429\n",
      "에폭 : 0047 비용 : 0.03787\n",
      "에폭 : 0048 비용 : 0.03486\n",
      "에폭 : 0049 비용 : 0.01906\n",
      "에폭 : 0050 비용 : 0.01832\n",
      "에폭 : 0051 비용 : 0.02119\n",
      "에폭 : 0052 비용 : 0.02385\n",
      "에폭 : 0053 비용 : 0.01835\n",
      "에폭 : 0054 비용 : 0.01806\n",
      "에폭 : 0055 비용 : 0.02285\n",
      "에폭 : 0056 비용 : 0.02677\n",
      "에폭 : 0057 비용 : 0.00945\n",
      "에폭 : 0058 비용 : 0.01913\n",
      "에폭 : 0059 비용 : 0.01839\n",
      "에폭 : 0060 비용 : 0.02503\n",
      "에폭 : 0061 비용 : 0.01190\n",
      "에폭 : 0062 비용 : 0.00799\n",
      "에폭 : 0063 비용 : 0.04186\n",
      "에폭 : 0064 비용 : 0.02510\n",
      "에폭 : 0065 비용 : 0.01129\n",
      "에폭 : 0066 비용 : 0.01080\n",
      "에폭 : 0067 비용 : 0.00811\n",
      "에폭 : 0068 비용 : 0.00443\n",
      "에폭 : 0069 비용 : 0.00907\n",
      "에폭 : 0070 비용 : 0.01125\n",
      "에폭 : 0071 비용 : 0.00715\n",
      "에폭 : 0072 비용 : 0.00570\n",
      "에폭 : 0073 비용 : 0.00880\n",
      "에폭 : 0074 비용 : 0.01908\n",
      "에폭 : 0075 비용 : 0.00619\n",
      "에폭 : 0076 비용 : 0.00503\n",
      "에폭 : 0077 비용 : 0.00749\n",
      "에폭 : 0078 비용 : 0.00517\n",
      "에폭 : 0079 비용 : 0.01696\n",
      "에폭 : 0080 비용 : 0.00587\n",
      "에폭 : 0081 비용 : 0.00524\n",
      "에폭 : 0082 비용 : 0.00467\n",
      "에폭 : 0083 비용 : 0.00240\n",
      "에폭 : 0084 비용 : 0.00564\n",
      "에폭 : 0085 비용 : 0.01104\n",
      "에폭 : 0086 비용 : 0.00308\n",
      "에폭 : 0087 비용 : 0.00322\n",
      "에폭 : 0088 비용 : 0.00412\n",
      "에폭 : 0089 비용 : 0.00456\n",
      "에폭 : 0090 비용 : 0.00217\n",
      "에폭 : 0091 비용 : 0.00387\n",
      "에폭 : 0092 비용 : 0.00500\n",
      "에폭 : 0093 비용 : 0.00416\n",
      "에폭 : 0094 비용 : 0.00683\n",
      "에폭 : 0095 비용 : 0.01175\n",
      "에폭 : 0096 비용 : 0.01189\n",
      "에폭 : 0097 비용 : 0.00169\n",
      "에폭 : 0098 비용 : 0.00293\n",
      "에폭 : 0099 비용 : 0.00504\n",
      "에폭 : 0100 비용 : 0.00229\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 학습\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch) :\n",
    "    _, cv = sess.run([opt, cost], feed_dict={encInput:input_batch ,decInput:output_batch, targets:target_batch})\n",
    "    \n",
    "    print('에폭 : %04d'%(epoch+1), '비용 : {:.5f}'.format(cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수\n",
    "def translate(w) :\n",
    "    seq_data = [w, 'P'*len(w)]  # ['word', 'PPPP']\n",
    "    \n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    # input_batch = ['w','o','r','d']\n",
    "    # output_batch = ['P','P','P','P']\n",
    "    # target_batch = [2,2,2,2]\n",
    "    \n",
    "    #                       0      1      2\n",
    "    # model실행 결과 : [배치사이즈, 스텝, 입력크기]\n",
    "    prediction = tf.argmax(model, 2)  # 2번째 차원인 입력크키에 argmax 적용 => 확률이 가장 높은 글자 예측\n",
    "    #                                        글자 인덱스\n",
    "    # [[[0, 0, 0.9, 0.5, ,,, 0.1]]]  => [[[2],[3],,,[14]]]\n",
    "    # ['단','어','E','E']\n",
    "    \n",
    "    res = sess.run(prediction, feed_dict={encInput:input_batch ,decInput:output_batch, targets:target_batch})\n",
    "    \n",
    "    decoded = [char_arr[i] for i in res[0]]\n",
    "    # E 제거\n",
    "    end = decoded.index('E')\n",
    "    \n",
    "    translated = ''.join(decoded[:end])\n",
    "    \n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word? 단어\n",
      "wodr? 나무\n",
      "love? 사랑\n",
      "loev? 사랑\n",
      "translate? 소소녀녀\n"
     ]
    }
   ],
   "source": [
    "print('word?', translate('word'))\n",
    "print('wodr?', translate('wodr'))\n",
    "print('love?', translate('love'))\n",
    "print('loev?', translate('loev'))\n",
    "print('translate?', translate('translate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
